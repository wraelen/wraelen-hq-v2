{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 4, "column": 0}, "map": {"version":3,"sources":["file:///home/wraelen/wraelen/wraelen-internal-boilerplate/src/lib/actions.ts"],"sourcesContent":["'use server'; // Logic: Marks as server-only (no client bundle bloat – optimizes for internal app with leads/calls)\nimport { PrismaClient } from '@prisma/client';  // Your DB client (async-safe in actions)\nimport { createServerClient } from '@supabase/ssr'; // SSR package (server-aware – auto-handles cookies via proxy; middleware refreshes post-redirect)\nimport axios from 'axios';  // API/fetch (lightweight – no fetch polyfill needed)\nimport * as cheerio from 'cheerio';  // HTML parse (fast/static – better than Puppeteer for Zillow; handles fallback)\nimport { cookies } from 'next/headers'; // Next utility (dynamic read for session check – set ignored in actions, as middleware handles)\nimport { redirect } from 'next/navigation'; // Server redirect (reliable – no client hacks; best for post-auth flow to dashboard quests)\nimport { chromium } from 'playwright'; // Logic: Headless browser (already in devDeps – executes JS for full page load)\nimport { z } from 'zod';  // Validation (type-safe inputs – prevents junk data in DB; no-brainer for prod)\nimport crypto from 'crypto';  // Built-in hash (no extra deps – for address_hash dedup)\nimport type { Database } from '../types/database.types'; // Types (autocompletes e.g., session.user.id for Prisma sync – now fixed via your gen)\n\n// Optional: Import Playwright for robust scraping (bypasses 403/Cloudflare – use if Axios fails; enable via env)\n\nconst supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!; // Logic: Required env (fail-fast if missing – matches middleware guard)\nconst supabaseAnonKey = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!;\nconst prisma = new PrismaClient();  // Global instance (efficient in Next.js actions – auto-closes; push back: Cache in lib/prisma.ts for hot reloads if issues)\n\nconst extractSchema = z.object({\n  links: z.string().min(1),\n  source: z.enum(['zillow', 'propstream', 'both']).default('zillow'),\n});  // Logic: Zod schema (validates formData – quick errors without DB hits)\n\n// Action: Sign out (logic: Centralized mutation – avoids serialization issues in layout; redirects to signin)\nexport async function signOutAction() {\n  const cookieStore = cookies();\n  const supabase = createServerClient<Database>(supabaseUrl, supabaseAnonKey, {\n    cookies: {\n      getAll: () => cookieStore.getAll(),\n      setAll: (cookiesToSet) => { try { cookiesToSet.forEach(({ name, value, options }) => cookieStore.set(name, value, options)); } catch {} },\n    },\n  });\n  await supabase.auth.signOut(); // Logic: Clears session (middleware will redirect unauthed requests)\n  redirect('/auth/signin'); // Logic: Post-signout flow (back to login – update to '/' if public landing needed)\n}\n\n// Action: Extract and merge from links (logic: Parse batch → per-link extract → upsert properties (merge on hash) → create leads (link FK, award points) – returns results for UX feedback)\nexport async function extractDataAction(formData: FormData) {\n  const validated = extractSchema.safeParse({\n    links: formData.get('links')?.toString() ?? '',\n    source: formData.get('source')?.toString() ?? 'zillow',\n  });  // Logic: Safe parse (returns { success, data } or { error } – client displays errors)\n\n  if (!validated.success) {\n    return { error: validated.error.format() };  // Logic: Return zod errors (e.g., \"Links required\" – no redirect)\n  }\n\n  const { links, source } = validated.data;\n  const linkArray = links.split(/[\\n,]/).map(l => l.trim()).filter(Boolean); // Logic: Handle comma or newline-separated (textarea-friendly); filter empties\n  const results = await Promise.allSettled(linkArray.map(async (link) => { // Logic: Parallel for speed; settled handles per-link failures without crashing batch\n    try {\n      const extracted = await extractFromLink(link, source);\n      if (!extracted.address) {\n        throw new Error('Failed to extract valid address – skipping link'); // Logic: Early guard (prevents bad hash/upsert)\n      }\n\n      const addressHash = crypto.createHash('sha256').update(extracted.address.toLowerCase()).digest('hex'); // Logic: Normalize case for dedup\n\n      // Logic: Use Prisma transaction for atomicity (upsert property + create lead + update points – prevents partial failures)\n      const [property, lead] = await prisma.$transaction(async (tx) => {\n        const prop = await tx.properties.upsert({ // Logic: Merge existing (e.g., update zestimate if newer)\n          where: { address_hash: addressHash },\n          update: {\n            address: extracted.address,\n            property_type: extracted.property_type || 'other',\n            bedrooms: extracted.bedrooms ?? null, // Logic: Nullables to avoid defaults if missing (match schema)\n            bathrooms: extracted.bathrooms ?? null,\n            square_feet: extracted.square_feet ?? null,\n            lot_size: extracted.lot_size ?? null,\n            year_built: extracted.year_built ?? null,\n            zestimate: extracted.zestimate ?? null,\n            avm: extracted.avm ?? null,\n            tax_assessed_value: extracted.tax_assessed_value ?? null,\n            distress_signals: extracted.distress_signals ?? {}, // JSONB default empty\n            owner_occupied: extracted.owner_occupied ?? null,\n            metadata: { ...extracted.metadata, sources: [...(extracted.metadata.sources || []), source] }, // Append sources for audit\n          },\n          create: {\n            address: extracted.address,\n            address_hash: addressHash,\n            property_type: extracted.property_type || 'other',\n            bedrooms: extracted.bedrooms ?? null,\n            bathrooms: extracted.bathrooms ?? null,\n            square_feet: extracted.square_feet ?? null,\n            lot_size: extracted.lot_size ?? null,\n            year_built: extracted.year_built ?? null,\n            zestimate: extracted.zestimate ?? null,\n            avm: extracted.avm ?? null,\n            tax_assessed_value: extracted.tax_assessed_value ?? null,\n            distress_signals: extracted.distress_signals ?? {},\n            owner_occupied: extracted.owner_occupied ?? null,\n            metadata: extracted.metadata,\n          },\n        });\n\n        // Fetch session for assignment (reuse Supabase client – consistent)\n        const cookieStore = cookies();\n        const supabase = createServerClient<Database>(supabaseUrl, supabaseAnonKey, {\n          cookies: {\n            getAll: () => cookieStore.getAll(),\n            setAll: (cookiesToSet) => { try { cookiesToSet.forEach(({ name, value, options }) => cookieStore.set(name, value, options)); } catch {} },\n          },\n        });\n        const { data: { session } } = await supabase.auth.getSession();\n        if (!session?.user.id) {\n          throw new Error('No active session – login required for assignment');\n        }\n\n        const ld = await tx.leads.create({\n          data: {\n            properties_id: prop.id,\n            lead_type: extracted.lead_type || 'owner',  // Infer (e.g., from distress_signals)\n            first_name: extracted.first_name ?? null,\n            last_name: extracted.last_name ?? null,\n            phone: extracted.phone ?? null,\n            source: extracted.source || source,\n            metadata: { original_link: link, extracted_data: extracted },\n            assigned_to: session.user.id,  // UUID from auth\n            points_earned: 10,  // Base; gamify more on status changes\n          },\n        });\n\n        // Gamification stub: Increment user points (expand to check/complete quests, award badges)\n        await tx.profile.update({\n          where: { id: session.user.id },\n          data: { points: { increment: 10 } }, // Logic: Atomic update (ties to XP bar/leaderboards)\n        });\n\n        return [prop, ld];\n      });\n\n      return { link, leadId: lead.id, success: true };\n    } catch (error) {\n      console.error(`Extraction error for ${link}:`, error); // Logic: Server log for debugging (view in Vercel/terminal)\n      return { link, success: false, error: (error as Error).message };\n    }\n  }));\n\n  return { results: results.map(r => r.status === 'fulfilled' ? r.value : { success: false, error: (r.reason as Error).message }) }; // Logic: Flatten for client display\n}\n\n// Helper: Extract from single link (logic: Source switch → API or scrape → normalize output – expandable for Propstream)\nasync function extractFromLink(link: string, source: 'zillow' | 'propstream' | 'both') {\n  let extracted: Record<string, any> = { source, metadata: { link, scrape_time: new Date().toISOString() } }; // Base output (json-friendly)\n\n  if (source === 'zillow' || source === 'both') {\n    const zillowKey = process.env.ZILLOW_API_KEY;\n    if (zillowKey) {\n      try {\n        // Logic: Bridge API (adjust auth if needed – docs use Authorization: Bearer; test with your key)\n        const response = await axios.get(`https://api.bridgeapi.io/v1/public-records/property?url=${encodeURIComponent(link)}`, {\n          headers: { 'Authorization': `Bearer ${zillowKey}` }, // Push back: Update if RapidAPI (rare for Bridge); confirm in docs\n        });\n        extracted = { ...extracted, ...response.data, source: 'zillow_api' }; // Merge (normalize fields below if API shape differs)\n      } catch (apiError) {\n        console.error(`Zillow API error for ${link}:`, apiError); // Fallback to scrape\n      }\n    }\n\n    if (!extracted.address) {  // Scrape fallback (improved headers to mimic browser – avoids 403; timeout to prevent hangs)\n      let html: string;\n      try {\n        const response = await axios.get(link, {\n          headers: {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36', // Logic: Current Chrome (2024/2025) – update periodically\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip, deflate, br',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1',\n            'Sec-Fetch-Dest': 'document',\n            'Sec-Fetch-Mode': 'navigate',\n            'Sec-Fetch-Site': 'none',\n            'Sec-Fetch-User': '?1',\n            'Referer': 'https://www.zillow.com/', // Logic: Fake referer – helps bypass some checks\n          },\n          timeout: 5000,\n        });\n        html = response.data;\n      } catch (axiosError) {\n        console.error(`Axios scrape failed for ${link} (likely 403):`, axiosError);\n        // Pushback: Fallback to Playwright if env enabled (headless browser – bypasses Cloudflare/JS detection; slower but reliable for Zillow)\n        if (process.env.USE_PLAYWRIGHT === 'true') {\n          const browser = await chromium.launch({ headless: true }); // Logic: Launch headless Chrome (add { args: ['--no-sandbox'] } if perms issues)\n          const page = await browser.newPage();\n          await page.goto(link, { waitUntil: 'networkidle' }); // Logic: Wait for load (handles JS)\n          html = await page.content();\n          await browser.close();\n        } else {\n          throw new Error('Scrape failed – enable USE_PLAYWRIGHT=true in .env.local for browser fallback');\n        }\n      }\n\n      const $ = cheerio.load(html);\n\n      // Improved: Parse preloaded JSON (reliable; Zillow embeds data here)\n      const preloadedData = $('script#hdpApolloPreloadedData').text();\n      if (preloadedData) {\n        try {\n          const jsonData = JSON.parse(preloadedData); // Logic: No need for decodeURIComponent – Zillow often serves plain JSON\n          // Logic: Navigate cache (dynamic key; find property object – Zillow structure as of 2025)\n          const cacheKeys = Object.keys(jsonData.apiCache || {});\n          const propertyKey = cacheKeys.find(key => key.includes('ForSaleDoubleScrollFullRenderQuery') || key.includes('Property')); // Logic: Flexible for changes\n          const propertyData = jsonData.apiCache[propertyKey]?.property || jsonData.property || {};\n\n          extracted.address = `${propertyData.streetAddress || ''}, ${propertyData.city || ''}, ${propertyData.state || ''} ${propertyData.zipcode || ''}`.trim();\n          extracted.zestimate = propertyData.zestimate || null;\n          extracted.bedrooms = propertyData.bedrooms || null;\n          extracted.bathrooms = propertyData.bathrooms || null;\n          extracted.square_feet = propertyData.livingArea || null;\n          extracted.lot_size = propertyData.lotAreaValue || null; // In sqft or acres; normalize if needed\n          extracted.year_built = propertyData.yearBuilt || null;\n          extracted.tax_assessed_value = propertyData.taxAssessedValue || null;\n          // Add more: e.g., extracted.property_type = propertyData.homeType?.toLowerCase();\n          // Note: Owner name/phone not in Zillow (privacy); use skiptrace API (e.g., TruePeopleSearch) post-extract for leads\n        } catch (parseError) {\n          console.error(`JSON parse error for ${link}:`, parseError);\n        }\n      }\n    }\n  }\n\n  if (source === 'propstream' || source === 'both') {\n    // Logic: Propstream no public scrape/API (TOS ban); stub for now – push back: Add form file input for CSV export upload\n    // If API key, integrate: e.g., await axios.post('https://api.propstream.com/v1/export', { links }, { headers: { 'Authorization': propstreamKey } });\n    extracted.avm = 0;  // Mock; replace with real\n    extracted.distress_signals = { pre_foreclosure: false };  // Mock (e.g., from CSV parse)\n    extracted.owner_occupied = true; // Mock\n    extracted.first_name = 'Mock'; // etc.\n  }\n\n  return extracted;  // Normalized (add more mappings as needed)\n}\n\n// ... (keep your signInAction)"],"names":[],"mappings":";;;;;;;IAqCsB,oBAAA,WAAA,GAAA,IAAA,+eAAA,EAAA,8CAAA,oeAAA,EAAA,KAAA,GAAA,0eAAA,EAAA","debugId":null}},
    {"offset": {"line": 16, "column": 0}, "map": {"version":3,"sources":["file:///home/wraelen/wraelen/wraelen-internal-boilerplate/src/app/extract/page.tsx"],"sourcesContent":["// src/app/extract/page.tsx – Extraction form (client-side for interactivity; binds server action – best hybrid for Next.js; protected via middleware)\n'use client';  // Logic: Client (hooks for state/form – no SSR overhead; push back: Full server if no batch UX needed)\n\nimport { zodResolver } from '@hookform/resolvers/zod';\nimport { useState } from 'react';\nimport { useForm } from 'react-hook-form';  // Dep not added yet? pnpm add react-hook-form @hookform/resolvers/zod (for zod integration)\nimport { z } from 'zod';  // Shared schema (client validation duplicates server for speed – prevents bad submits)\nimport { extractDataAction } from '@/lib/actions';  // Imported action\n\nconst extractSchema = z.object({  // Dupe from action (push back: Extract to shared util if bloat)\n  links: z.string().min(1, 'Enter at least one link'),\n  source: z.enum(['zillow', 'propstream', 'both']),\n});  // Type: Infer for form\ntype FormData = z.infer<typeof extractSchema>;\n\nexport default function ExtractPage() {\n  const [results, setResults] = useState<any[]>([]);  // Post-submit feedback (success/errors)\n  const [error, setError] = useState<string | null>(null);\n  const { register, handleSubmit, formState: { errors, isSubmitting } } = useForm<FormData>({\n    resolver: zodResolver(extractSchema),\n    defaultValues: { source: 'zillow' },\n  });\n\n  const onSubmit = async (data: FormData) => {\n    setError(null);\n    setResults([]);\n    const formData = new FormData();  // Logic: Native for action (though useForm has data – adaptable)\n    formData.append('links', data.links);\n    formData.append('source', data.source);\n    const result = await extractDataAction(formData);  // Call action\n\n    if (result.error) {\n      setError('Validation failed: ' + JSON.stringify(result.error));  // Display zod\n    } else {\n      setResults(result.results || []);  // Show batch outcomes (e.g., \"Lead X imported\")\n    }\n  };\n\n  return (\n    <div className=\"flex min-h-screen items-center justify-center bg-black text-green-400 font-mono\">\n      <form onSubmit={handleSubmit(onSubmit)} className=\"p-8 border-2 border-green-500 rounded-lg shadow-[0_0_15px_rgba(0,255,0,0.7)] bg-black/80 w-96\">\n        <h2 className=\"text-2xl mb-6 text-center\">Extract Leads Quest</h2>\n        <textarea\n          {...register('links')}\n          placeholder=\"Enter Zillow/Propstream links (comma-separated for batch)\"\n          className=\"w-full mb-4 p-2 bg-black border border-green-500 text-green-400 focus:outline-none focus:border-green-300\"\n        />\n        {errors.links && <p className=\"text-red-500 mb-4\">{errors.links.message}</p>}\n        <select {...register('source')} className=\"w-full mb-6 p-2 bg-black border border-green-500 text-green-400 focus:outline-none focus:border-green-300\">\n          <option value=\"zillow\">Zillow</option>\n          <option value=\"propstream\">Propstream</option>\n          <option value=\"both\">Both</option>\n        </select>\n        <button type=\"submit\" disabled={isSubmitting} className=\"w-full p-2 bg-green-500 text-black hover:bg-green-600\">\n          {isSubmitting ? 'Extracting...' : 'Start Extraction'}\n        </button>\n        {error && <p className=\"text-red-500 mt-4\">{error}</p>}\n        {results.length > 0 && (\n          <ul className=\"mt-4\">\n            {results.map((res, i) => (\n              <li key={i} className={res.success ? 'text-green-400' : 'text-red-500'}>\n                {res.success ? `Lead ${res.leadId} imported!` : `Error on link: ${res.error}`}\n              </li>\n            ))}\n          </ul>\n        )}\n      </form>\n    </div>\n  );\n}"],"names":[],"mappings":"AAAA,sJAAsJ;;;;;;AAGtJ;AACA;AACA,wfAA4C,4FAA4F;AACxI,8cAA0B,uFAAuF;AACjH,kVAAoD,kBAAkB;AANtE,cAAe,uGAAuG;;;;;;;AAQtH,MAAM,gBAAgB,oRAAC,CAAC,MAAM,CAAC;IAC7B,OAAO,oRAAC,CAAC,MAAM,GAAG,GAAG,CAAC,GAAG;IACzB,QAAQ,oRAAC,CAAC,IAAI,CAAC;QAAC;QAAU;QAAc;KAAO;AACjD,IAAK,uBAAuB;AAGb,SAAS;IACtB,MAAM,CAAC,SAAS,WAAW,GAAG,IAAA,idAAQ,EAAQ,EAAE,GAAI,wCAAwC;IAC5F,MAAM,CAAC,OAAO,SAAS,GAAG,IAAA,idAAQ,EAAgB;IAClD,MAAM,EAAE,QAAQ,EAAE,YAAY,EAAE,WAAW,EAAE,MAAM,EAAE,YAAY,EAAE,EAAE,GAAG,IAAA,mTAAO,EAAW;QACxF,UAAU,IAAA,+VAAW,EAAC;QACtB,eAAe;YAAE,QAAQ;QAAS;IACpC;IAEA,MAAM,WAAW,OAAO;QACtB,SAAS;QACT,WAAW,EAAE;QACb,MAAM,WAAW,IAAI,YAAa,iEAAiE;QACnG,SAAS,MAAM,CAAC,SAAS,KAAK,KAAK;QACnC,SAAS,MAAM,CAAC,UAAU,KAAK,MAAM;QACrC,MAAM,SAAS,MAAM,IAAA,wNAAiB,EAAC,WAAY,cAAc;QAEjE,IAAI,OAAO,KAAK,EAAE;YAChB,SAAS,wBAAwB,KAAK,SAAS,CAAC,OAAO,KAAK,IAAK,cAAc;QACjF,OAAO;YACL,WAAW,OAAO,OAAO,IAAI,EAAE,GAAI,gDAAgD;QACrF;IACF;IAEA,qBACE,8eAAC;QAAI,WAAU;kBACb,cAAA,8eAAC;YAAK,UAAU,aAAa;YAAW,WAAU;;8BAChD,8eAAC;oBAAG,WAAU;8BAA4B;;;;;;8BAC1C,8eAAC;oBACE,GAAG,SAAS,QAAQ;oBACrB,aAAY;oBACZ,WAAU;;;;;;gBAEX,OAAO,KAAK,kBAAI,8eAAC;oBAAE,WAAU;8BAAqB,OAAO,KAAK,CAAC,OAAO;;;;;;8BACvE,8eAAC;oBAAQ,GAAG,SAAS,SAAS;oBAAE,WAAU;;sCACxC,8eAAC;4BAAO,OAAM;sCAAS;;;;;;sCACvB,8eAAC;4BAAO,OAAM;sCAAa;;;;;;sCAC3B,8eAAC;4BAAO,OAAM;sCAAO;;;;;;;;;;;;8BAEvB,8eAAC;oBAAO,MAAK;oBAAS,UAAU;oBAAc,WAAU;8BACrD,eAAe,kBAAkB;;;;;;gBAEnC,uBAAS,8eAAC;oBAAE,WAAU;8BAAqB;;;;;;gBAC3C,QAAQ,MAAM,GAAG,mBAChB,8eAAC;oBAAG,WAAU;8BACX,QAAQ,GAAG,CAAC,CAAC,KAAK,kBACjB,8eAAC;4BAAW,WAAW,IAAI,OAAO,GAAG,mBAAmB;sCACrD,IAAI,OAAO,GAAG,CAAC,KAAK,EAAE,IAAI,MAAM,CAAC,UAAU,CAAC,GAAG,CAAC,eAAe,EAAE,IAAI,KAAK,EAAE;2BADtE;;;;;;;;;;;;;;;;;;;;;AASvB","debugId":null}}]
}